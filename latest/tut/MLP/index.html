<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="no-js ie6"><![endif]-->
<!--[if IE 7 ]><html class="no-js ie7"><![endif]-->
<!--[if IE 8 ]><html class="no-js ie8"><![endif]-->
<!--[if IE 9 ]><html class="no-js ie9"><![endif]-->
<!--[if (gt IE 9)|!(IE)]><!--> <html class="no-js"> <!--<![endif]-->
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    
      
        <title>Multi-Layer Perceptron - Latte.jl</title>
      
      
      
      
        <meta name="author" content="Intel Labs">
      
    
    <meta property="og:url" content="None">
    <meta property="og:title" content="Latte.jl">
    <meta property="og:image" content="None/../../">
    <meta name="apple-mobile-web-app-title" content="Latte.jl">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    
    
    <link rel="shortcut icon" type="image/x-icon" href="../../None">
    <link rel="icon" type="image/x-icon" href="../../None">
    <style>
      @font-face {
      	font-family: 'Icon';
      	src: url('../../assets/fonts/icon.eot?52m981');
      	src: url('../../assets/fonts/icon.eot?#iefix52m981')
               format('embedded-opentype'),
      		   url('../../assets/fonts/icon.woff?52m981')
               format('woff'),
      		   url('../../assets/fonts/icon.ttf?52m981')
               format('truetype'),
      		   url('../../assets/fonts/icon.svg?52m981#icon')
               format('svg');
      	font-weight: normal;
      	font-style: normal;
      }
    </style>
    <link rel="stylesheet" href="../../assets/stylesheets/application-54f87043f3.css">
    
      <link rel="stylesheet" href="../../assets/stylesheets/palettes-05ab2406df.css">
    
    
      
      
      
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu+Mono">
      <style>
        body, input {
          font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
        }
        pre, code {
          font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
        }
      </style>
    
    
      <link rel="stylesheet" href="../../assets/Documenter.css">
    
    <script src="../../assets/javascripts/modernizr-4ab42b99fd.js"></script>
    
  </head>
  
  
  
  <body class="palette-primary-blue palette-accent-light-blue">
    
      
      
    
    <div class="backdrop">
      <div class="backdrop-paper"></div>
    </div>
    <input class="toggle" type="checkbox" id="toggle-drawer">
    <input class="toggle" type="checkbox" id="toggle-search">
    <label class="toggle-button overlay" for="toggle-drawer"></label>
    <header class="header">
      <nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        
          <span class="path">
            
              
                Tutorial <i class="icon icon-link"></i>
              
            
          </span>
        
        Multi-Layer Perceptron
      </div>
    </div>
    
    
    <div class="button button-search" role="button" aria-label="Search">
      <label class="toggle-button icon icon-search" title="Search" for="toggle-search"></label>
    </div>
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
    </header>
    <main class="main">
      
      <div class="drawer">
        <nav aria-label="Navigation">
  
  <a href="https://github.com/IntelLabs/Latte.jl" class="project">
    <div class="banner">
      
      <div class="name">
        <strong>
          Latte.jl
          <span class="version">
            
          </span>
        </strong>
        
          <br>
          IntelLabs/Latte.jl
        
      </div>
    </div>
  </a>
  <div class="scrollable">
    <div class="wrapper">
      
        <ul class="repo">
          <li class="repo-download">
            
            <a href="https://github.com/IntelLabs/Latte.jl/archive/master.zip" target="_blank" title="Download" data-action="download">
              <i class="icon icon-download"></i> Download
            </a>
          </li>
          <li class="repo-stars">
            <a href="https://github.com/IntelLabs/Latte.jl/stargazers" target="_blank" title="Stargazers" data-action="star">
              <i class="icon icon-star"></i> Stars
              <span class="count">&ndash;</span>
            </a>
          </li>
        </ul>
        <hr>
      
      <div class="toc">
        <ul>
          
            
  <li>
    <a class="" title="Home" href="../..">
      Home
    </a>
    
  </li>

          
            
  <li>
    <a class="" title="Setup" href="../../setup/">
      Setup
    </a>
    
  </li>

          
            
  <li>
    <span class="section">Tutorial</span>
    <ul>
      
        
  <li>
    <a class="current" title="Multi-Layer Perceptron" href="./">
      Multi-Layer Perceptron
    </a>
    
      
        
      
      
        <ul>
          
            <li class="anchor">
              <a title="The Model" href="#the-model">
                The Model
              </a>
            </li>
          
            <li class="anchor">
              <a title="Defining a WeightedNeuron" href="#defining-a-weightedneuron">
                Defining a WeightedNeuron
              </a>
            </li>
          
            <li class="anchor">
              <a title="Building a Layer using Ensembles and Connections" href="#building-a-layer-using-ensembles-and-connections">
                Building a Layer using Ensembles and Connections
              </a>
            </li>
          
            <li class="anchor">
              <a title="Constructing an MLP using Net" href="#constructing-an-mlp-using-net">
                Constructing an MLP using Net
              </a>
            </li>
          
            <li class="anchor">
              <a title="Training" href="#training">
                Training
              </a>
            </li>
          
        </ul>
      
    
  </li>

      
    </ul>
  </li>

          
            
  <li>
    <span class="section">Manual</span>
    <ul>
      
        
  <li>
    <span class="section">Examples</span>
    <ul>
      
        
  <li>
    <a class="" title="MNIST" href="../../man/examples/mnist/">
      MNIST
    </a>
    
  </li>

      
        
  <li>
    <a class="" title="CIFAR-10" href="../../man/examples/cifar/">
      CIFAR-10
    </a>
    
  </li>

      
    </ul>
  </li>

      
    </ul>
  </li>

          
            
  <li>
    <span class="section">Library</span>
    <ul>
      
        
  <li>
    <a class="" title="Public" href="../../lib/public/">
      Public
    </a>
    
  </li>

      
        
  <li>
    <a class="" title="Internals" href="../../lib/internals/">
      Internals
    </a>
    
  </li>

      
    </ul>
  </li>

          
        </ul>
        
      </div>
    </div>
  </div>
</nav>
      </div>
      <article class="article">
        <div class="wrapper">
          
          <p><a id='Multi-Layer-Perceptron-1'></a></p>
<h1 id="multi-layer-perceptron">Multi-Layer Perceptron</h1>
<p>This tutorial is based on <a href="http://deeplearning.net/tutorial/mlp.html">http://deeplearning.net/tutorial/mlp.html</a> but uses Latte to implement the code examples.  </p>
<p>A Multi-Layer Perceptron can be described as a logistic regression classifier where the input is first transformed using a learnt non-linear transformation $\Phi$.  This intermediate layer performing the transformation is referred to as a <strong>hidden layer</strong>.  One hidden layer is sufficient to make MLPs a <strong>universal approximator</strong> [<a href="http://www.sciencedirect.com/science/article/pii/0893608089900208" title="Multilayer feedforward networks are universal approximators. Hornik et al. 1989">1</a>].  </p>
<p><a id='The-Model-1'></a></p>
<h2 id="the-model">The Model</h2>
<p>An MLP with a single hidden layer can be represented graphically as follows:</p>
<p><img alt="Single Hidden Layer MLP" src="http://deeplearning.net/tutorial/_images/mlp.png" /></p>
<p>To understand this representation, we'll first define the properties of a singular neuron.  A <strong>Neuron</strong>, depicted in the figure as a circle, computes an output (typically called an activation) as a function of its inputs.  In this figure, an input is depicted as a directed edge flowing into a <strong>Neuron</strong>.  For an MLP, the function to compute the output of a <strong>Neuron</strong> begins with a weighted sum of the inputs.  Formally, if we describe the input as a vector of values $x$ and a vector of weights $w$, this operation can be written as $w \cdot x$ (dot product).  Next, we will shift this value by a learned bias $b$, then apply an activation function $s$ such as $tanh$, $sigmoid$ or $relu$.  The entire computation is written as $s(w \cdot x + b)$.  The values of $w$ and $b$ will be learnt using back-propogation.</p>
<p><a id='Defining-a-WeightedNeuron-1'></a></p>
<h2 id="defining-a-weightedneuron">Defining a WeightedNeuron</h2>
<p>Defining a <strong>WeightedNeuron</strong> begins with a subtype of the abstract <code>Neuron</code> type.  The <code>Neuron</code> type contains 4 default fields: - <code>value</code>    – contains the output value of the neuron - <code>∇</code>        – contains the gradient of the neuron - <code>inputs</code>   – a vector of vectors of input values. - <code>∇inputs</code>  – a vector of gradients for connected neurons</p>
<p>For our <strong>WeightedNeuron</strong> we will define the following additional fields:</p>
<ul>
<li><code>weights</code>  – a vector of learned weights</li>
<li><code>∇weights</code> – a vector of gradients for the weights</li>
<li><code>bias</code>     – the bias value</li>
<li><code>∇bias</code>    – the gradient for the bias value</li>
</ul>
<div class="codehilite"><pre><span></span><span class="p">@</span><span class="n">neuron</span> <span class="k">type</span><span class="nc"> WeightedNeuron</span>
    <span class="n">weights</span>  <span class="p">::</span> <span class="n">DenseArray</span><span class="p">{</span><span class="kt">Float32</span><span class="p">}</span>
    <span class="n">∇weights</span> <span class="p">::</span> <span class="n">DenseArray</span><span class="p">{</span><span class="kt">Float32</span><span class="p">}</span>

    <span class="n">bias</span>     <span class="p">::</span> <span class="n">DenseArray</span><span class="p">{</span><span class="kt">Float32</span><span class="p">}</span>
    <span class="n">∇bias</span>    <span class="p">::</span> <span class="n">DenseArray</span><span class="p">{</span><span class="kt">Float32</span><span class="p">}</span>
<span class="k">end</span>
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We do not define the default fields as using the @neuron macro for the type definition will specify them for us.  Furthermore the macro defines a constructor function that automatically initializes the default fields.</p>
</div>
<p>Next we define the forward computation for the neuron:</p>
<div class="codehilite"><pre><span></span><span class="p">@</span><span class="n">neuron</span> <span class="n">forward</span><span class="p">(</span><span class="n">neuron</span><span class="p">::</span><span class="n">WeightedNeuron</span><span class="p">)</span> <span class="k">do</span>
    <span class="c"># dot product of weights and inputs</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="p">:</span><span class="n">length</span><span class="p">(</span><span class="n">neuron</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">neuron</span><span class="o">.</span><span class="n">value</span> <span class="o">+=</span> <span class="n">neuron</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">neuron</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
    <span class="k">end</span>
    <span class="c"># shift by the bias value</span>
    <span class="n">neuron</span><span class="o">.</span><span class="n">value</span> <span class="o">+=</span> <span class="n">neuron</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">end</span>
</pre></div>


<p>And finally we define the backward computation for the back-propogation algorithm:</p>
<div class="codehilite"><pre><span></span><span class="p">@</span><span class="n">neuron</span> <span class="n">backward</span><span class="p">(</span><span class="n">neuron</span><span class="p">::</span><span class="n">WeightedNeuron</span><span class="p">)</span> <span class="k">do</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="p">:</span><span class="n">length</span><span class="p">(</span><span class="n">neuron</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">neuron</span><span class="o">.</span><span class="n">∇inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">neuron</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">neuron</span><span class="o">.</span><span class="n">∇</span>
    <span class="k">end</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="p">:</span><span class="n">length</span><span class="p">(</span><span class="n">neuron</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">neuron</span><span class="o">.</span><span class="n">∇weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">neuron</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">neuron</span><span class="o">.</span><span class="n">∇</span>
    <span class="k">end</span>
    <span class="n">neuron</span><span class="o">.</span><span class="n">∇bias</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">neuron</span><span class="o">.</span><span class="n">∇</span>
<span class="k">end</span>
</pre></div>


<p><a id='Building-a-Layer-using-Ensembles-and-Connections-1'></a></p>
<h2 id="building-a-layer-using-ensembles-and-connections">Building a Layer using Ensembles and Connections</h2>
<p>In Latte, a <em>layer</em> can be described as an <code>Ensemble</code> of <code>Neuron</code>s with a specific set of connections to one or more input <code>Ensemble</code>s.  To construct a <strong>Hidden Layer</strong> for our MLP, we will use an <code>Ensemble</code> of <code>WeightedNeurons</code> with each neuron connected to each all the neurons in the input <code>Ensemble</code>.</p>
<p>Our <code>FullyConnectedLayer</code> will be a Julia Function that instantiates an <code>Ensemble</code> of <code>WeightedNeuron</code>s and applies a full connection structure to the <code>input_ensemble</code>.  The signature looks like this:</p>
<div class="codehilite"><pre><span></span><span class="k">function</span><span class="nf"> FCLayer</span><span class="p">(</span><span class="n">name</span><span class="p">::</span><span class="n">Symbol</span><span class="p">,</span> <span class="n">net</span><span class="p">::</span><span class="n">Net</span><span class="p">,</span> <span class="n">input_ensemble</span><span class="p">::</span><span class="n">AbstractEnsemble</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">::</span><span class="kt">Int</span><span class="p">)</span>
</pre></div>


<p>To construct a hidden layer with <code>num_neurons</code>, we begin by instantiating a 1-d <code>Array</code> to hold our <code>WeightedNeurons</code>.</p>
<div class="codehilite"><pre><span></span>    <span class="n">neurons</span> <span class="o">=</span> <span class="n">Array</span><span class="p">(</span><span class="n">WeightedNeuron</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">)</span>
</pre></div>


<p>Next we instantiate the parameters for our <code>WeightedNeurons</code>.  Note <code>xavier</code> refers to a function to initialize a random set of values using the <em>Xavier</em> (TODO: Reference) initialization scheme.  <code>xavier</code> and other initialization routines are provided as part of the Latte standard library.</p>
<div class="codehilite"><pre><span></span>    <span class="n">num_inputs</span> <span class="o">=</span> <span class="n">length</span><span class="p">(</span><span class="n">input_ensemble</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">xavier</span><span class="p">(</span><span class="kt">Float32</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">)</span>
    <span class="n">∇weights</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="kt">Float32</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">)</span>

    <span class="n">bias</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="kt">Float32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">)</span>
    <span class="n">∇bias</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="kt">Float32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">)</span>
</pre></div>


<p>With our parameters initialized, we are ready to initialize our neurons.  Note that each <code>WeightedNeuron</code> instance uses a different row of parameter values.</p>
<div class="codehilite"><pre><span></span>    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="p">:</span><span class="n">num_neurons</span>
        <span class="n">neurons</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">WeightedNeuron</span><span class="p">(</span><span class="n">view</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">),</span> <span class="n">view</span><span class="p">(</span><span class="n">∇weights</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">),</span>
                                    <span class="n">view</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">),</span> <span class="n">view</span><span class="p">(</span><span class="n">∇bias</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">))</span>
    <span class="k">end</span>
</pre></div>


<p>Finally, we are ready to instantiate our Ensemble.</p>
<div class="codehilite"><pre><span></span>    ens = Ensemble(net, name, neurons, [Param(name,:weights, 1.0f0, 1.0f0), 
                                        Param(name,:bias, 2.0f0, 0.0f0)])
</pre></div>


<p>Then we add connections to each neuron in <code>input_ensemble</code></p>
<div class="codehilite"><pre><span></span>    <span class="n">add_connections</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">input_ensemble</span><span class="p">,</span> <span class="n">ens</span><span class="p">,</span>
                    <span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">([</span><span class="n">Colon</span><span class="p">()</span> <span class="k">for</span> <span class="n">d</span> <span class="k">in</span> <span class="n">size</span><span class="p">(</span><span class="n">input_ensemble</span><span class="p">)]</span><span class="o">...</span> <span class="p">)))</span>
</pre></div>


<p>Finally, we return the constructed Ensemble so it can be used as an input to another layer.</p>
<div class="codehilite"><pre><span></span>    return ens
end
</pre></div>


<p><a id='Constructing-an-MLP-using-Net-1'></a></p>
<h2 id="constructing-an-mlp-using-net">Constructing an MLP using Net</h2>
<p>To construct an MLP we instantiate the <code>Net</code> type with a batch size of $100$.  Then we use the Latte standard library provided <code>HDF5DataLayer</code> that constructs an input ensemble that reads from <code>HDF5</code> datasets.  (TODO: Link to explanation of Latte's HDF5 format).  Then we construct two <code>FCLayer</code>s using the function that we defined.  Finally we use two more Latte standard library layers as output layers.  The <code>SoftmaxLoss</code> layer is used for train the network and the <code>AccuracyLayer</code> is used for test the network.</p>
<div class="codehilite"><pre><span></span><span class="k">using</span> <span class="n">Latte</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">HDF5DataLayer</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="s">&quot;data/train.txt&quot;</span><span class="p">,</span> <span class="s">&quot;data/test.txt&quot;</span><span class="p">)</span>

<span class="n">fc1</span> <span class="o">=</span> <span class="n">FCLayer</span><span class="p">(:</span><span class="n">fc1</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">fc2</span> <span class="o">=</span> <span class="n">FCLayer</span><span class="p">(:</span><span class="n">fc2</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">fc1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="n">loss</span>     <span class="o">=</span> <span class="n">SoftmaxLossLayer</span><span class="p">(:</span><span class="n">loss</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">fc2</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">AccuracyLayer</span><span class="p">(:</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">fc2</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="n">SolverParameters</span><span class="p">(</span>
    <span class="n">lr_policy</span>    <span class="o">=</span> <span class="n">LRPolicy</span><span class="o">.</span><span class="n">Inv</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">),</span>
    <span class="n">mom_policy</span>   <span class="o">=</span> <span class="n">MomPolicy</span><span class="o">.</span><span class="n">Fixed</span><span class="p">(</span><span class="mf">0.9</span><span class="p">),</span>
    <span class="n">max_epoch</span>    <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">regu_coef</span>    <span class="o">=</span> <span class="o">.</span><span class="mi">0005</span><span class="p">)</span>
<span class="n">sgd</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">solve</span><span class="p">(</span><span class="n">sgd</span><span class="p">,</span> <span class="n">net</span><span class="p">)</span>
</pre></div>


<p><a id='Training-1'></a></p>
<h2 id="training">Training</h2>
<p>We will train the above MLP on the MNIST digit recognition dataset.  For your convenience the code in this tutorial has been provided in <code>tutorials/mlp/mlp.jl</code>.  Note that the name <code>WeightedNeuron</code> was replaced with <code>MLPNeuron</code> to resolve conflicts with the existing <code>WeightedNeuron</code> definition in the Latte standard library.  To train the network, first download and convert the dataset by running <code>tutorials/mlp/data/get-data.sh</code>.  Then train by running the script <code>julia mlp.jl</code>.  You should the following output that shows the loss values and test results:</p>
<div class="codehilite"><pre><span></span>...
INFO: 07-Apr 15:15:22 - Entering solve loop
INFO: 07-Apr 15:15:23 - Iter 20 - Loss: 1.4688001
INFO: 07-Apr 15:15:24 - Iter 40 - Loss: 0.6913204
INFO: 07-Apr 15:15:25 - Iter 60 - Loss: 0.6053091
INFO: 07-Apr 15:15:26 - Iter 80 - Loss: 0.6043377
INFO: 07-Apr 15:15:27 - Iter 100 - Loss: 0.57204634
INFO: 07-Apr 15:15:28 - Iter 120 - Loss: 0.500179
INFO: 07-Apr 15:15:28 - Iter 140 - Loss: 0.40663132
INFO: 07-Apr 15:15:29 - Iter 160 - Loss: 0.3704785
INFO: 07-Apr 15:15:29 - Iter 180 - Loss: 0.3620596
INFO: 07-Apr 15:15:30 - Iter 200 - Loss: 0.46897307
INFO: 07-Apr 15:15:30 - Iter 220 - Loss: 0.45075363
INFO: 07-Apr 15:15:31 - Iter 240 - Loss: 0.3376474
INFO: 07-Apr 15:15:31 - Iter 260 - Loss: 0.5301368
INFO: 07-Apr 15:15:32 - Iter 280 - Loss: 0.28490248
INFO: 07-Apr 15:15:32 - Iter 300 - Loss: 0.33110633
INFO: 07-Apr 15:15:33 - Iter 320 - Loss: 0.26910272
INFO: 07-Apr 15:15:33 - Iter 340 - Loss: 0.32226878
INFO: 07-Apr 15:15:33 - Iter 360 - Loss: 0.3838666
INFO: 07-Apr 15:15:34 - Iter 380 - Loss: 0.24588501
INFO: 07-Apr 15:15:34 - Iter 400 - Loss: 0.4209111
INFO: 07-Apr 15:15:35 - Iter 420 - Loss: 0.25582874
INFO: 07-Apr 15:15:35 - Iter 440 - Loss: 0.3958639
INFO: 07-Apr 15:15:36 - Iter 460 - Loss: 0.27812394
INFO: 07-Apr 15:15:36 - Iter 480 - Loss: 0.45379284
INFO: 07-Apr 15:15:37 - Iter 500 - Loss: 0.35272872
INFO: 07-Apr 15:15:38 - Iter 520 - Loss: 0.39787623
INFO: 07-Apr 15:15:38 - Iter 540 - Loss: 0.30763283
INFO: 07-Apr 15:15:39 - Iter 560 - Loss: 0.35435736
INFO: 07-Apr 15:15:40 - Iter 580 - Loss: 0.33140996
INFO: 07-Apr 15:15:41 - Iter 600 - Loss: 0.34410283
INFO: 07-Apr 15:15:41 - Epoch 1 - Testing...
INFO: 07-Apr 15:15:44 - Epoch 1 - Test Result: 90.88118%
...
</pre></div>
          <aside class="copyright" role="note">
            
            Documentation built with
            <a href="http://www.mkdocs.org" target="_blank">MkDocs</a>
            using the
            <a href="http://squidfunk.github.io/mkdocs-material/" target="_blank">
              Material
            </a>
            theme.
          </aside>
          
            <footer class="footer">
              
  <nav class="pagination" aria-label="Footer">
    <div class="previous">
      
        <a href="../../setup/" title="Setup">
          <span class="direction">
            Previous
          </span>
          <div class="page">
            <div class="button button-previous" role="button" aria-label="Previous">
              <i class="icon icon-back"></i>
            </div>
            <div class="stretch">
              <div class="title">
                Setup
              </div>
            </div>
          </div>
        </a>
      
    </div>
    <div class="next">
      
        <a href="../../man/examples/mnist/" title="MNIST">
          <span class="direction">
            Next
          </span>
          <div class="page">
            <div class="stretch">
              <div class="title">
                MNIST
              </div>
            </div>
            <div class="button button-next" role="button" aria-label="Next">
              <i class="icon icon-forward"></i>
            </div>
          </div>
        </a>
      
    </div>
  </nav>

            </footer>
          
        </div>
      </article>
      <div class="results" role="status" aria-live="polite">
        <div class="scrollable">
          <div class="wrapper">
            <div class="meta"></div>
            <div class="list"></div>
          </div>
        </div>
      </div>
    </main>
    <script>
      var base_url = '../..';
      var repo_id  = 'IntelLabs/Latte.jl';
    </script>
    <script src="../../assets/javascripts/application-997097ee0c.js"></script>
    
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
      <script src="../../assets/mathjaxhelper.js"></script>
    
    
  </body>
</html>