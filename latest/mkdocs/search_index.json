{
    "docs": [
        {
            "location": "/", 
            "text": "Latte.jl\n\n\nLatte is a high-performance DSL for deep neural networks.  Our goal was to design a language that closely matched domain concepts without sacrificing performance.  \n\n\n\n\nNote\n\n\nPlease note that Latte is prelease (alpha) software and this site is a work     in progress.  We are working hard on filling out this site with     documentation, tutorials, and examples.  Please report bugs using the     \nGithub issue tracker\n. For     usage questions, visit the      \nGitter room\n.\n\n\n\n\nIf you use Latte for research, please use the following citation:\n\n\nForthcoming...", 
            "title": "Home"
        }, 
        {
            "location": "/#lattejl", 
            "text": "Latte is a high-performance DSL for deep neural networks.  Our goal was to design a language that closely matched domain concepts without sacrificing performance.     Note  Please note that Latte is prelease (alpha) software and this site is a work     in progress.  We are working hard on filling out this site with     documentation, tutorials, and examples.  Please report bugs using the      Github issue tracker . For     usage questions, visit the       Gitter room .   If you use Latte for research, please use the following citation:  Forthcoming...", 
            "title": "Latte.jl"
        }, 
        {
            "location": "/setup/", 
            "text": "Setup\n\n\n\n\nPrerequisites\n\n\nTo build Latte, you will need HDF5 and cmake.  What to install will vary by platform and your needs.  On Ubuntu, try:\n\n\n$ sudo apt-get install hdf5-tools libhdf5-dev cmake\n\n\n\n\n\nLatte currently depends on \nIntel MKL\n and the \nIntel C++ Compiler (icpc)\n.\n\n\n\n\nQuick Install\n\n\n# Latte currently depends on the master branch of these packages\n\n\njulia\n \nPkg\n.\nclone\n(\nhttps://github.com/IntelLabs/Latte.jl\n)\n\n\njulia\n \nPkg\n.\ncheckout\n(\nCompilerTools\n)\n\n\njulia\n \nPkg\n.\ncheckout\n(\nParallelAccelerator\n)\n\n\njulia\n \n# To build with MPI enabled, uncomment these lines\n\n\njulia\n \n# ENV[\nLATTE_BUILD_MPI\n] = 1\n\n\njulia\n \n# ENV[\nCXX\n] = \nmpiicpc\n  # Replace with your mpi compiler wrapper\n\n\njulia\n \nPkg\n.\nbuild\n(\nLatte\n)", 
            "title": "Setup"
        }, 
        {
            "location": "/setup/#setup", 
            "text": "", 
            "title": "Setup"
        }, 
        {
            "location": "/setup/#prerequisites", 
            "text": "To build Latte, you will need HDF5 and cmake.  What to install will vary by platform and your needs.  On Ubuntu, try:  $ sudo apt-get install hdf5-tools libhdf5-dev cmake  Latte currently depends on  Intel MKL  and the  Intel C++ Compiler (icpc) .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/setup/#quick-install", 
            "text": "# Latte currently depends on the master branch of these packages  julia   Pkg . clone ( https://github.com/IntelLabs/Latte.jl )  julia   Pkg . checkout ( CompilerTools )  julia   Pkg . checkout ( ParallelAccelerator )  julia   # To build with MPI enabled, uncomment these lines  julia   # ENV[ LATTE_BUILD_MPI ] = 1  julia   # ENV[ CXX ] =  mpiicpc   # Replace with your mpi compiler wrapper  julia   Pkg . build ( Latte )", 
            "title": "Quick Install"
        }, 
        {
            "location": "/tut/MLP/", 
            "text": "Multi-Layer Perceptron\n\n\nThis tutorial is based on \nhttp://deeplearning.net/tutorial/mlp.html\n but uses Latte to implement the code examples.  \n\n\nA Multi-Layer Perceptron (MLP) can be described as a logistic regression classifier where the input is first transformed using a learnt non-linear transformation $\\Phi$.  This intermediate layer performing the transformation is referred to as a \nhidden layer\n.  One hidden layer is sufficient to make MLPs a \nuniversal approximator\n [\n1\n].  \n\n\n\n\nThe Model\n\n\nAn MLP with a single hidden layer can be represented graphically as follows:\n\n\n\n\nTo understand this representation, we'll first define the properties of a singular neuron.  A \nNeuron\n, depicted in the figure as a circle, computes an output (typically called an activation) as a function of its inputs.  In this figure, an input is depicted as a directed edge flowing into a \nNeuron\n.  For an MLP, the function to compute the output of a \nNeuron\n begins with a weighted sum of the inputs.  Formally, if we describe the input as a vector of values $x$ and a vector of weights $w$, this operation can be written as $w \\cdot x$ (dot product).  Next, we will shift this value by a learned bias $b$, then apply an activation function $s$ such as $tanh$, $sigmoid$ or $relu$.  The entire computation is written as $s(w \\cdot x + b)$.\n\n\n\n\nBackpropogation\n\n\nForthcoming...\n\n\n\n\nStochastic Gradient Descent\n\n\nGradient Descent is a simple algorithm that repeatedly makes small steps downward on an error surface defined by a loss function of some parameters. Traditional Gradient Descent computes the gradient after a pass over the entire input data set.  In practice, Gradient Descent proceeds more quickly when the gradient is estimated from just a few examples at time.  This extreme form computes the gradient for a single training input at a time:\n\n\nfor\n \n(\ninput\n,\n \nexpected_result\n)\n \nin\n \ntraining_set\n:\n\n    \nloss\n \n=\n \nf\n(\nparams\n,\n \ninput\n,\n \nexpected_result\n)\n\n    \n\u2207_loss_wrt_params\n \n=\n \n...\n  \n# compute gradients\n\n    \nparams\n \n-=\n \nlearning_rate\n \n*\n \n\u2207_loss_wrt_params\n\n    \nif\n \nstop_condition_met\n()\n\n        \nreturn\n \nparams\n\n    \nend\n\n\nend\n\n\n\n\n\n\nIn practice, SGD for deep learning is performed using \nminibatches\n, where a \nminibatch\n of inputs are used to estimate the gradients.  This technique typically reduces variance in the estimation of the gradient, but more importantly allows implementations to make better use of the memory hierarchy in modern computers.\n\n\n\n\nDefining a WeightedNeuron\n\n\nDefining a \nWeightedNeuron\n begins with a subtype of the abstract \nNeuron\n type.  The \nNeuron\n type contains 4 default fields:\n\n\n\n\nvalue\n    \u2013 contains the output value of the neuron\n\n\n\u2207\n        \u2013 contains the gradient of the neuron\n\n\ninputs\n   \u2013 a vector of vectors of input values.\n\n\n\u2207inputs\n  \u2013 a vector of gradients for connected neurons\n\n\n\n\nFor our \nWeightedNeuron\n we will define the following additional fields:\n\n\n\n\nweights\n  \u2013 a vector of learned weights\n\n\n\u2207weights\n \u2013 a vector of gradients for the weights\n\n\nbias\n     \u2013 the bias value\n\n\n\u2207bias\n    \u2013 the gradient for the bias value\n\n\n\n\n@\nneuron\n \ntype\n WeightedNeuron\n\n    \nweights\n  \n::\n \nDenseArray\n{\nFloat32\n}\n\n    \n\u2207weights\n \n::\n \nDenseArray\n{\nFloat32\n}\n\n\n    \nbias\n     \n::\n \nDenseArray\n{\nFloat32\n}\n\n    \n\u2207bias\n    \n::\n \nDenseArray\n{\nFloat32\n}\n\n\nend\n\n\n\n\n\n\n\n\nNote\n\n\nWe do not define the default fields as using the @neuron macro for the type definition will specify them for us.  Furthermore the macro defines a constructor function that automatically initializes the default fields.\n\n\n\n\nNext we define the forward computation for the neuron:\n\n\n@\nneuron\n \nforward\n(\nneuron\n::\nWeightedNeuron\n)\n \ndo\n\n    \n# dot product of weights and inputs\n\n    \nfor\n \ni\n \nin\n \n1\n:\nlength\n(\nneuron\n.\ninputs\n[\n1\n])\n\n        \nneuron\n.\nvalue\n \n+=\n \nneuron\n.\nweights\n[\ni\n]\n \n*\n \nneuron\n.\ninputs\n[\n1\n][\ni\n]\n\n    \nend\n\n    \n# shift by the bias value\n\n    \nneuron\n.\nvalue\n \n+=\n \nneuron\n.\nbias\n[\n1\n]\n\n\nend\n\n\n\n\n\n\nAnd finally we define the backward computation for the back-propogation algorithm:\n\n\n@\nneuron\n \nbackward\n(\nneuron\n::\nWeightedNeuron\n)\n \ndo\n\n    \nfor\n \ni\n \nin\n \n1\n:\nlength\n(\nneuron\n.\ninputs\n[\n1\n])\n\n        \nneuron\n.\n\u2207inputs\n[\n1\n][\ni\n]\n \n+=\n \nneuron\n.\nweights\n[\ni\n]\n \n*\n \nneuron\n.\n\u2207\n\n    \nend\n\n    \nfor\n \ni\n \nin\n \n1\n:\nlength\n(\nneuron\n.\ninputs\n[\n1\n])\n\n        \nneuron\n.\n\u2207weights\n[\ni\n]\n \n+=\n \nneuron\n.\ninputs\n[\n1\n][\ni\n]\n \n*\n \nneuron\n.\n\u2207\n\n    \nend\n\n    \nneuron\n.\n\u2207bias\n[\n1\n]\n \n+=\n \nneuron\n.\n\u2207\n\n\nend\n\n\n\n\n\n\n\n\nBuilding a Layer using Ensembles and Connections\n\n\nIn Latte, a \nlayer\n can be described as an \nEnsemble\n of \nNeuron\ns with a specific set of connections to one or more input \nEnsemble\ns.  To construct a \nHidden Layer\n for our MLP, we will use an \nEnsemble\n of \nWeightedNeurons\n with each neuron connected to each all the neurons in the input \nEnsemble\n.\n\n\nOur \nFullyConnectedLayer\n will be a Julia Function that instantiates an \nEnsemble\n of \nWeightedNeuron\ns and applies a full connection structure to the \ninput_ensemble\n.  The signature looks like this:\n\n\nfunction\n FCLayer\n(\nname\n::\nSymbol\n,\n \nnet\n::\nNet\n,\n \ninput_ensemble\n::\nAbstractEnsemble\n,\n \nnum_neurons\n::\nInt\n)\n\n\n\n\n\n\nTo construct a hidden layer with \nnum_neurons\n, we begin by instantiating a 1-d \nArray\n to hold our \nWeightedNeurons\n.\n\n\n    \nneurons\n \n=\n \nArray\n(\nWeightedNeuron\n,\n \nnum_neurons\n)\n\n\n\n\n\n\nNext we instantiate the parameters for our \nWeightedNeurons\n.  Note \nxavier\n refers to a function to initialize a random set of values using the \nXavier\n (TODO: Reference) initialization scheme.  \nxavier\n and other initialization routines are provided as part of the Latte standard library.\n\n\n    \nnum_inputs\n \n=\n \nlength\n(\ninput_ensemble\n)\n\n    \nweights\n \n=\n \nxavier\n(\nFloat32\n,\n \nnum_inputs\n,\n \nnum_neurons\n)\n\n    \n\u2207weights\n \n=\n \nzeros\n(\nFloat32\n,\n \nnum_inputs\n,\n \nnum_neurons\n)\n\n\n    \nbias\n \n=\n \nzeros\n(\nFloat32\n,\n \n1\n,\n \nnum_neurons\n)\n\n    \n\u2207bias\n \n=\n \nzeros\n(\nFloat32\n,\n \n1\n,\n \nnum_neurons\n)\n\n\n\n\n\n\nWith our parameters initialized, we are ready to initialize our neurons.  Note that each \nWeightedNeuron\n instance uses a different row of parameter values.\n\n\n    \nfor\n \ni\n \nin\n \n1\n:\nnum_neurons\n\n        \nneurons\n[\ni\n]\n \n=\n \nWeightedNeuron\n(\nview\n(\nweights\n,\n \n:,\n \ni\n),\n \nview\n(\n\u2207weights\n,\n \n:,\n \ni\n),\n\n                                    \nview\n(\nbias\n,\n \n:,\n \ni\n),\n \nview\n(\n\u2207bias\n,\n \n:,\n \ni\n))\n\n    \nend\n\n\n\n\n\n\nFinally, we are ready to instantiate our Ensemble.\n\n\n    ens = Ensemble(net, name, neurons, [Param(name,:weights, 1.0f0, 1.0f0), \n                                        Param(name,:bias, 2.0f0, 0.0f0)])\n\n\n\n\n\nThen we add connections to each neuron in \ninput_ensemble\n\n\n    \nadd_connections\n(\nnet\n,\n \ninput_ensemble\n,\n \nens\n,\n\n                    \n(\ni\n)\n \n-\n \n(\ntuple\n([\nColon\n()\n \nfor\n \nd\n \nin\n \nsize\n(\ninput_ensemble\n)]\n...\n \n)))\n\n\n\n\n\n\nFinally, we return the constructed Ensemble so it can be used as an input to another layer.\n\n\n    return ens\nend\n\n\n\n\n\n\n\nConstructing an MLP using Net\n\n\nTo construct an MLP we instantiate the \nNet\n type with a batch size of $100$.  Then we use the Latte standard library provided \nHDF5DataLayer\n that constructs an input ensemble that reads from \nHDF5\n datasets.  (TODO: Link to explanation of Latte's HDF5 format).  Then we construct two \nFCLayer\ns using the function that we defined.  Finally we use two more Latte standard library layers as output layers.  The \nSoftmaxLoss\n layer is used for train the network and the \nAccuracyLayer\n is used for test the network.\n\n\nusing\n \nLatte\n\n\n\nnet\n \n=\n \nNet\n(\n100\n)\n\n\ndata\n,\n \nlabel\n \n=\n \nHDF5DataLayer\n(\nnet\n,\n \ndata/train.txt\n,\n \ndata/test.txt\n)\n\n\n\nfc1\n \n=\n \nFCLayer\n(:\nfc1\n,\n \nnet\n,\n \ndata\n,\n \n100\n)\n\n\nfc2\n \n=\n \nFCLayer\n(:\nfc2\n,\n \nnet\n,\n \nfc1\n,\n \n10\n)\n\n\n\nloss\n     \n=\n \nSoftmaxLossLayer\n(:\nloss\n,\n \nnet\n,\n \nfc2\n,\n \nlabel\n)\n\n\naccuracy\n \n=\n \nAccuracyLayer\n(:\naccuracy\n,\n \nnet\n,\n \nfc2\n,\n \nlabel\n)\n\n\n\nparams\n \n=\n \nSolverParameters\n(\n\n    \nlr_policy\n    \n=\n \nLRPolicy\n.\nInv\n(\n0.01\n,\n \n0.0001\n,\n \n0.75\n),\n\n    \nmom_policy\n   \n=\n \nMomPolicy\n.\nFixed\n(\n0.9\n),\n\n    \nmax_epoch\n    \n=\n \n50\n,\n\n    \nregu_coef\n    \n=\n \n.\n0005\n)\n\n\nsgd\n \n=\n \nSGD\n(\nparams\n)\n\n\nsolve\n(\nsgd\n,\n \nnet\n)\n\n\n\n\n\n\n\n\nTraining\n\n\nWe will train the above MLP on the MNIST digit recognition dataset.  For your convenience the code in this tutorial has been provided in \ntutorials/mlp/mlp.jl\n.  Note that the name \nWeightedNeuron\n was replaced with \nMLPNeuron\n to resolve conflicts with the existing \nWeightedNeuron\n definition in the Latte standard library.  To train the network, first download and convert the dataset by running \ntutorials/mlp/data/get-data.sh\n.  Then train by running the script \njulia mlp.jl\n.  You should the following output that shows the loss values and test results:\n\n\n...\nINFO: 07-Apr 15:15:22 - Entering solve loop\nINFO: 07-Apr 15:15:23 - Iter 20 - Loss: 1.4688001\nINFO: 07-Apr 15:15:24 - Iter 40 - Loss: 0.6913204\nINFO: 07-Apr 15:15:25 - Iter 60 - Loss: 0.6053091\nINFO: 07-Apr 15:15:26 - Iter 80 - Loss: 0.6043377\nINFO: 07-Apr 15:15:27 - Iter 100 - Loss: 0.57204634\nINFO: 07-Apr 15:15:28 - Iter 120 - Loss: 0.500179\nINFO: 07-Apr 15:15:28 - Iter 140 - Loss: 0.40663132\nINFO: 07-Apr 15:15:29 - Iter 160 - Loss: 0.3704785\nINFO: 07-Apr 15:15:29 - Iter 180 - Loss: 0.3620596\nINFO: 07-Apr 15:15:30 - Iter 200 - Loss: 0.46897307\nINFO: 07-Apr 15:15:30 - Iter 220 - Loss: 0.45075363\nINFO: 07-Apr 15:15:31 - Iter 240 - Loss: 0.3376474\nINFO: 07-Apr 15:15:31 - Iter 260 - Loss: 0.5301368\nINFO: 07-Apr 15:15:32 - Iter 280 - Loss: 0.28490248\nINFO: 07-Apr 15:15:32 - Iter 300 - Loss: 0.33110633\nINFO: 07-Apr 15:15:33 - Iter 320 - Loss: 0.26910272\nINFO: 07-Apr 15:15:33 - Iter 340 - Loss: 0.32226878\nINFO: 07-Apr 15:15:33 - Iter 360 - Loss: 0.3838666\nINFO: 07-Apr 15:15:34 - Iter 380 - Loss: 0.24588501\nINFO: 07-Apr 15:15:34 - Iter 400 - Loss: 0.4209111\nINFO: 07-Apr 15:15:35 - Iter 420 - Loss: 0.25582874\nINFO: 07-Apr 15:15:35 - Iter 440 - Loss: 0.3958639\nINFO: 07-Apr 15:15:36 - Iter 460 - Loss: 0.27812394\nINFO: 07-Apr 15:15:36 - Iter 480 - Loss: 0.45379284\nINFO: 07-Apr 15:15:37 - Iter 500 - Loss: 0.35272872\nINFO: 07-Apr 15:15:38 - Iter 520 - Loss: 0.39787623\nINFO: 07-Apr 15:15:38 - Iter 540 - Loss: 0.30763283\nINFO: 07-Apr 15:15:39 - Iter 560 - Loss: 0.35435736\nINFO: 07-Apr 15:15:40 - Iter 580 - Loss: 0.33140996\nINFO: 07-Apr 15:15:41 - Iter 600 - Loss: 0.34410283\nINFO: 07-Apr 15:15:41 - Epoch 1 - Testing...\nINFO: 07-Apr 15:15:44 - Epoch 1 - Test Result: 90.88118%\n...", 
            "title": "Multi-Layer Perceptron"
        }, 
        {
            "location": "/tut/MLP/#multi-layer-perceptron", 
            "text": "This tutorial is based on  http://deeplearning.net/tutorial/mlp.html  but uses Latte to implement the code examples.    A Multi-Layer Perceptron (MLP) can be described as a logistic regression classifier where the input is first transformed using a learnt non-linear transformation $\\Phi$.  This intermediate layer performing the transformation is referred to as a  hidden layer .  One hidden layer is sufficient to make MLPs a  universal approximator  [ 1 ].", 
            "title": "Multi-Layer Perceptron"
        }, 
        {
            "location": "/tut/MLP/#the-model", 
            "text": "An MLP with a single hidden layer can be represented graphically as follows:   To understand this representation, we'll first define the properties of a singular neuron.  A  Neuron , depicted in the figure as a circle, computes an output (typically called an activation) as a function of its inputs.  In this figure, an input is depicted as a directed edge flowing into a  Neuron .  For an MLP, the function to compute the output of a  Neuron  begins with a weighted sum of the inputs.  Formally, if we describe the input as a vector of values $x$ and a vector of weights $w$, this operation can be written as $w \\cdot x$ (dot product).  Next, we will shift this value by a learned bias $b$, then apply an activation function $s$ such as $tanh$, $sigmoid$ or $relu$.  The entire computation is written as $s(w \\cdot x + b)$.", 
            "title": "The Model"
        }, 
        {
            "location": "/tut/MLP/#backpropogation", 
            "text": "Forthcoming...", 
            "title": "Backpropogation"
        }, 
        {
            "location": "/tut/MLP/#stochastic-gradient-descent", 
            "text": "Gradient Descent is a simple algorithm that repeatedly makes small steps downward on an error surface defined by a loss function of some parameters. Traditional Gradient Descent computes the gradient after a pass over the entire input data set.  In practice, Gradient Descent proceeds more quickly when the gradient is estimated from just a few examples at time.  This extreme form computes the gradient for a single training input at a time:  for   ( input ,   expected_result )   in   training_set : \n     loss   =   f ( params ,   input ,   expected_result ) \n     \u2207_loss_wrt_params   =   ...    # compute gradients \n     params   -=   learning_rate   *   \u2207_loss_wrt_params \n     if   stop_condition_met () \n         return   params \n     end  end   In practice, SGD for deep learning is performed using  minibatches , where a  minibatch  of inputs are used to estimate the gradients.  This technique typically reduces variance in the estimation of the gradient, but more importantly allows implementations to make better use of the memory hierarchy in modern computers.", 
            "title": "Stochastic Gradient Descent"
        }, 
        {
            "location": "/tut/MLP/#defining-a-weightedneuron", 
            "text": "Defining a  WeightedNeuron  begins with a subtype of the abstract  Neuron  type.  The  Neuron  type contains 4 default fields:   value     \u2013 contains the output value of the neuron  \u2207         \u2013 contains the gradient of the neuron  inputs    \u2013 a vector of vectors of input values.  \u2207inputs   \u2013 a vector of gradients for connected neurons   For our  WeightedNeuron  we will define the following additional fields:   weights   \u2013 a vector of learned weights  \u2207weights  \u2013 a vector of gradients for the weights  bias      \u2013 the bias value  \u2207bias     \u2013 the gradient for the bias value   @ neuron   type  WeightedNeuron \n     weights    ::   DenseArray { Float32 } \n     \u2207weights   ::   DenseArray { Float32 } \n\n     bias       ::   DenseArray { Float32 } \n     \u2207bias      ::   DenseArray { Float32 }  end    Note  We do not define the default fields as using the @neuron macro for the type definition will specify them for us.  Furthermore the macro defines a constructor function that automatically initializes the default fields.   Next we define the forward computation for the neuron:  @ neuron   forward ( neuron :: WeightedNeuron )   do \n     # dot product of weights and inputs \n     for   i   in   1 : length ( neuron . inputs [ 1 ]) \n         neuron . value   +=   neuron . weights [ i ]   *   neuron . inputs [ 1 ][ i ] \n     end \n     # shift by the bias value \n     neuron . value   +=   neuron . bias [ 1 ]  end   And finally we define the backward computation for the back-propogation algorithm:  @ neuron   backward ( neuron :: WeightedNeuron )   do \n     for   i   in   1 : length ( neuron . inputs [ 1 ]) \n         neuron . \u2207inputs [ 1 ][ i ]   +=   neuron . weights [ i ]   *   neuron . \u2207 \n     end \n     for   i   in   1 : length ( neuron . inputs [ 1 ]) \n         neuron . \u2207weights [ i ]   +=   neuron . inputs [ 1 ][ i ]   *   neuron . \u2207 \n     end \n     neuron . \u2207bias [ 1 ]   +=   neuron . \u2207  end", 
            "title": "Defining a WeightedNeuron"
        }, 
        {
            "location": "/tut/MLP/#building-a-layer-using-ensembles-and-connections", 
            "text": "In Latte, a  layer  can be described as an  Ensemble  of  Neuron s with a specific set of connections to one or more input  Ensemble s.  To construct a  Hidden Layer  for our MLP, we will use an  Ensemble  of  WeightedNeurons  with each neuron connected to each all the neurons in the input  Ensemble .  Our  FullyConnectedLayer  will be a Julia Function that instantiates an  Ensemble  of  WeightedNeuron s and applies a full connection structure to the  input_ensemble .  The signature looks like this:  function  FCLayer ( name :: Symbol ,   net :: Net ,   input_ensemble :: AbstractEnsemble ,   num_neurons :: Int )   To construct a hidden layer with  num_neurons , we begin by instantiating a 1-d  Array  to hold our  WeightedNeurons .       neurons   =   Array ( WeightedNeuron ,   num_neurons )   Next we instantiate the parameters for our  WeightedNeurons .  Note  xavier  refers to a function to initialize a random set of values using the  Xavier  (TODO: Reference) initialization scheme.   xavier  and other initialization routines are provided as part of the Latte standard library.       num_inputs   =   length ( input_ensemble ) \n     weights   =   xavier ( Float32 ,   num_inputs ,   num_neurons ) \n     \u2207weights   =   zeros ( Float32 ,   num_inputs ,   num_neurons ) \n\n     bias   =   zeros ( Float32 ,   1 ,   num_neurons ) \n     \u2207bias   =   zeros ( Float32 ,   1 ,   num_neurons )   With our parameters initialized, we are ready to initialize our neurons.  Note that each  WeightedNeuron  instance uses a different row of parameter values.       for   i   in   1 : num_neurons \n         neurons [ i ]   =   WeightedNeuron ( view ( weights ,   :,   i ),   view ( \u2207weights ,   :,   i ), \n                                     view ( bias ,   :,   i ),   view ( \u2207bias ,   :,   i )) \n     end   Finally, we are ready to instantiate our Ensemble.      ens = Ensemble(net, name, neurons, [Param(name,:weights, 1.0f0, 1.0f0), \n                                        Param(name,:bias, 2.0f0, 0.0f0)])  Then we add connections to each neuron in  input_ensemble       add_connections ( net ,   input_ensemble ,   ens , \n                     ( i )   -   ( tuple ([ Colon ()   for   d   in   size ( input_ensemble )] ...   )))   Finally, we return the constructed Ensemble so it can be used as an input to another layer.      return ens\nend", 
            "title": "Building a Layer using Ensembles and Connections"
        }, 
        {
            "location": "/tut/MLP/#constructing-an-mlp-using-net", 
            "text": "To construct an MLP we instantiate the  Net  type with a batch size of $100$.  Then we use the Latte standard library provided  HDF5DataLayer  that constructs an input ensemble that reads from  HDF5  datasets.  (TODO: Link to explanation of Latte's HDF5 format).  Then we construct two  FCLayer s using the function that we defined.  Finally we use two more Latte standard library layers as output layers.  The  SoftmaxLoss  layer is used for train the network and the  AccuracyLayer  is used for test the network.  using   Latte  net   =   Net ( 100 )  data ,   label   =   HDF5DataLayer ( net ,   data/train.txt ,   data/test.txt )  fc1   =   FCLayer (: fc1 ,   net ,   data ,   100 )  fc2   =   FCLayer (: fc2 ,   net ,   fc1 ,   10 )  loss       =   SoftmaxLossLayer (: loss ,   net ,   fc2 ,   label )  accuracy   =   AccuracyLayer (: accuracy ,   net ,   fc2 ,   label )  params   =   SolverParameters ( \n     lr_policy      =   LRPolicy . Inv ( 0.01 ,   0.0001 ,   0.75 ), \n     mom_policy     =   MomPolicy . Fixed ( 0.9 ), \n     max_epoch      =   50 , \n     regu_coef      =   . 0005 )  sgd   =   SGD ( params )  solve ( sgd ,   net )", 
            "title": "Constructing an MLP using Net"
        }, 
        {
            "location": "/tut/MLP/#training", 
            "text": "We will train the above MLP on the MNIST digit recognition dataset.  For your convenience the code in this tutorial has been provided in  tutorials/mlp/mlp.jl .  Note that the name  WeightedNeuron  was replaced with  MLPNeuron  to resolve conflicts with the existing  WeightedNeuron  definition in the Latte standard library.  To train the network, first download and convert the dataset by running  tutorials/mlp/data/get-data.sh .  Then train by running the script  julia mlp.jl .  You should the following output that shows the loss values and test results:  ...\nINFO: 07-Apr 15:15:22 - Entering solve loop\nINFO: 07-Apr 15:15:23 - Iter 20 - Loss: 1.4688001\nINFO: 07-Apr 15:15:24 - Iter 40 - Loss: 0.6913204\nINFO: 07-Apr 15:15:25 - Iter 60 - Loss: 0.6053091\nINFO: 07-Apr 15:15:26 - Iter 80 - Loss: 0.6043377\nINFO: 07-Apr 15:15:27 - Iter 100 - Loss: 0.57204634\nINFO: 07-Apr 15:15:28 - Iter 120 - Loss: 0.500179\nINFO: 07-Apr 15:15:28 - Iter 140 - Loss: 0.40663132\nINFO: 07-Apr 15:15:29 - Iter 160 - Loss: 0.3704785\nINFO: 07-Apr 15:15:29 - Iter 180 - Loss: 0.3620596\nINFO: 07-Apr 15:15:30 - Iter 200 - Loss: 0.46897307\nINFO: 07-Apr 15:15:30 - Iter 220 - Loss: 0.45075363\nINFO: 07-Apr 15:15:31 - Iter 240 - Loss: 0.3376474\nINFO: 07-Apr 15:15:31 - Iter 260 - Loss: 0.5301368\nINFO: 07-Apr 15:15:32 - Iter 280 - Loss: 0.28490248\nINFO: 07-Apr 15:15:32 - Iter 300 - Loss: 0.33110633\nINFO: 07-Apr 15:15:33 - Iter 320 - Loss: 0.26910272\nINFO: 07-Apr 15:15:33 - Iter 340 - Loss: 0.32226878\nINFO: 07-Apr 15:15:33 - Iter 360 - Loss: 0.3838666\nINFO: 07-Apr 15:15:34 - Iter 380 - Loss: 0.24588501\nINFO: 07-Apr 15:15:34 - Iter 400 - Loss: 0.4209111\nINFO: 07-Apr 15:15:35 - Iter 420 - Loss: 0.25582874\nINFO: 07-Apr 15:15:35 - Iter 440 - Loss: 0.3958639\nINFO: 07-Apr 15:15:36 - Iter 460 - Loss: 0.27812394\nINFO: 07-Apr 15:15:36 - Iter 480 - Loss: 0.45379284\nINFO: 07-Apr 15:15:37 - Iter 500 - Loss: 0.35272872\nINFO: 07-Apr 15:15:38 - Iter 520 - Loss: 0.39787623\nINFO: 07-Apr 15:15:38 - Iter 540 - Loss: 0.30763283\nINFO: 07-Apr 15:15:39 - Iter 560 - Loss: 0.35435736\nINFO: 07-Apr 15:15:40 - Iter 580 - Loss: 0.33140996\nINFO: 07-Apr 15:15:41 - Iter 600 - Loss: 0.34410283\nINFO: 07-Apr 15:15:41 - Epoch 1 - Testing...\nINFO: 07-Apr 15:15:44 - Epoch 1 - Test Result: 90.88118%\n...", 
            "title": "Training"
        }, 
        {
            "location": "/man/examples/mnist/", 
            "text": "MNIST\n\n\n\n\nTLDR\n\n\n$ \ncd\n ~/.julia/v0.4/Latte/examples/mnist/data\n$ ./get-data.sh\n$ \ncd\n ..\n$ julia mnist.jl\n\n\n\n\n\n\n\nPreparing the data\n\n\nThese steps can be performed automatically by running the \nget-data.sh\n script inside the \nexamples/mnist\n directory.\n\n\nFirst we will download the MNIST dataset from Yann Lecun's website.\n\n\nTARGET_DIR\n=\n$(\npwd\n)\n\n\nfor\n dset in train-images-idx3-ubyte.gz train-labels-idx1-ubyte.gz \n\\\n\n    t10k-images-idx3-ubyte.gz t10k-labels-idx1-ubyte.gz\n\ndo\n\n    curl -o \n$TARGET_DIR\n/\n$dset\n -O http://yann.lecun.com/exdb/mnist/\n$dset\n\n    \nSTEM\n=\n$(\nbasename \n${\ndset\n}\n .gz\n)\n\n    gunzip -c \n$TARGET_DIR\n/\n$dset\n \n \n$TARGET_DIR\n/\n$STEM\n\n\ndone\n\n\n\n\n\n\nNext we will convert the binary data into HDF5 datasets.  This code is contained in \nconvert.jl\n.\n\n\nThis begins by reading the binary files using Julia's \nopen\n.\n\n\nbase_dir = \n./\n\n\n\ndatasets = Dict(\ntrain\n =\n [\n$\nbase_dir\n/train-labels-idx1-ubyte\n,\n$\nbase_dir\n/train-images-idx3-ubyte\n],\n\n\n                \ntest\n =\n [\n$\nbase_dir\n/t10k-labels-idx1-ubyte\n,\n$\nbase_dir\n/t10k-images-idx3-ubyte\n])\n\n\n\nfor key in keys(datasets)\n\n\n  label_fn, data_fn = datasets[key]\n\n\n  label_f = open(label_fn)\n\n\n  data_f  = open(data_fn)\n\n\n\n\n\n\nNext we read the headers for the binary data to give us information about the dataset dimensions.\n\n\n  label_header = read(label_f, Int32, 2)\n  @assert ntoh(label_header[1]) == 2049\n  n_label = round(Int, ntoh(label_header[2]))\n  data_header = read(data_f, Int32, 4)\n  @assert ntoh(data_header[1]) == 2051\n  n_data = round(Int, ntoh(data_header[2]))\n  @assert n_label == n_data\n  h = round(Int, ntoh(data_header[3]))\n  w = round(Int, ntoh(data_header[4]))\n\n\n\n\n\nNext we open an HDF5 file for writing and initialize two datasets (label and data).\n\n\n  println(\nExporting \n$\nn_data\n digits of size \n$\nh\n x \n$\nw\n)\n\n\n\n  h5open(\n$\nbase_dir\n/\n$\nkey\n.\nhdf5\n, \nw\n) do h5\n\n\n    dset_data = d_create(h5, \ndata\n, datatype(Float32), dataspace(w, h, 1, n_data))\n\n\n    dset_label = d_create(h5, \nlabel\n, datatype(Float32), dataspace(1, n_data))\n\n\n\n\n\n\nThen we read the label and data bytes and convert them to Arrays of Float32.  We normalize the data to values between [0, 1) by dividing by 256.\n\n\n    img = readbytes(data_f, n_data * h*w)\n    img = convert(Array{Float32},img) / 256 # scale into [0,1)\n    class = readbytes(label_f, n_data)\n    class = convert(Array{Float32},class)\n\n\n\n\n\nWe will permute the indices of the dataset so that they are stored in a shuffled ordering.  Then we iterate over the permuted indices and store the data and label values into the HDF5 dataset.\n\n\n    idx = 1:n_data\n\n\n    println(\n  \n$\nidx\n...\n)\n\n\n\n    idx = collect(idx)\n\n\n    rp = randperm(length(idx))\n\n\n\n    for j = 1:length(idx)\n\n\n      r_idx = rp[j]\n\n\n      dset_data[:,:,1,idx[j]] = img[(r_idx-1)*h*w+1:r_idx*h*w]\n\n\n      dset_label[1,idx[j]] = class[r_idx]\n\n\n    end\n\n\n\n\n\n\n\n\nThe Model\n\n\nThe model code can be found in \nexamples/mnist.jl\n\n\nusing Latte\n\nnet = Net(100)\ndata, label = HDF5DataLayer(net, \ndata/train.txt\n, \ndata/test.txt\n)\nconv1    = ConvolutionLayer(:conv1, net, data, 20, 5, 1, 1)\nrelu1    = ReLULayer(:relu1, net, conv1)\npool1    = MaxPoolingLayer(:pool1, net, relu1, 2, 2, 0)\nconv2    = ConvolutionLayer(:conv2, net, pool1, 50, 5, 1, 1)\nrelu2    = ReLULayer(:relu2, net, conv2)\npool2    = MaxPoolingLayer(:pool2, net, relu2, 2, 2, 0)\nconv3    = ConvolutionLayer(:conv3, net, pool1, 50, 3, 1, 1)\nrelu3    = ReLULayer(:relu3, net, conv3)\npool3    = MaxPoolingLayer(:pool3, net, relu3, 2, 2, 0)\nfc4      = InnerProductLayer(:fc4, net, pool3, 512)\nrelu4    = ReLULayer(:relu4, net, fc4)\nfc5      = InnerProductLayer(:fc5, net, relu4, 512)\nrelu5    = ReLULayer(:relu5, net, fc5)\nfc6      = InnerProductLayer(:fc6, net, relu5, 10)\nloss     = SoftmaxLossLayer(:loss, net, fc6, label)\naccuracy = AccuracyLayer(:accuracy, net, fc6, label)\n\nparams = SolverParameters(\n    lr_policy    = LRPolicy.Inv(0.01, 0.0001, 0.75),\n    mom_policy   = MomPolicy.Fixed(0.9),\n    max_epoch    = 50,\n    regu_coef    = .0005)\nsgd = SGD(params)\nsolve(sgd, net)", 
            "title": "MNIST"
        }, 
        {
            "location": "/man/examples/mnist/#mnist", 
            "text": "", 
            "title": "MNIST"
        }, 
        {
            "location": "/man/examples/mnist/#tldr", 
            "text": "$  cd  ~/.julia/v0.4/Latte/examples/mnist/data\n$ ./get-data.sh\n$  cd  ..\n$ julia mnist.jl", 
            "title": "TLDR"
        }, 
        {
            "location": "/man/examples/mnist/#preparing-the-data", 
            "text": "These steps can be performed automatically by running the  get-data.sh  script inside the  examples/mnist  directory.  First we will download the MNIST dataset from Yann Lecun's website.  TARGET_DIR = $( pwd )  for  dset in train-images-idx3-ubyte.gz train-labels-idx1-ubyte.gz  \\ \n    t10k-images-idx3-ubyte.gz t10k-labels-idx1-ubyte.gz do \n    curl -o  $TARGET_DIR / $dset  -O http://yann.lecun.com/exdb/mnist/ $dset \n     STEM = $( basename  ${ dset }  .gz ) \n    gunzip -c  $TARGET_DIR / $dset     $TARGET_DIR / $STEM  done   Next we will convert the binary data into HDF5 datasets.  This code is contained in  convert.jl .  This begins by reading the binary files using Julia's  open .  base_dir =  ./  datasets = Dict( train  =  [ $ base_dir /train-labels-idx1-ubyte , $ base_dir /train-images-idx3-ubyte ],                   test  =  [ $ base_dir /t10k-labels-idx1-ubyte , $ base_dir /t10k-images-idx3-ubyte ])  for key in keys(datasets)    label_fn, data_fn = datasets[key]    label_f = open(label_fn)    data_f  = open(data_fn)   Next we read the headers for the binary data to give us information about the dataset dimensions.    label_header = read(label_f, Int32, 2)\n  @assert ntoh(label_header[1]) == 2049\n  n_label = round(Int, ntoh(label_header[2]))\n  data_header = read(data_f, Int32, 4)\n  @assert ntoh(data_header[1]) == 2051\n  n_data = round(Int, ntoh(data_header[2]))\n  @assert n_label == n_data\n  h = round(Int, ntoh(data_header[3]))\n  w = round(Int, ntoh(data_header[4]))  Next we open an HDF5 file for writing and initialize two datasets (label and data).    println( Exporting  $ n_data  digits of size  $ h  x  $ w )    h5open( $ base_dir / $ key . hdf5 ,  w ) do h5      dset_data = d_create(h5,  data , datatype(Float32), dataspace(w, h, 1, n_data))      dset_label = d_create(h5,  label , datatype(Float32), dataspace(1, n_data))   Then we read the label and data bytes and convert them to Arrays of Float32.  We normalize the data to values between [0, 1) by dividing by 256.      img = readbytes(data_f, n_data * h*w)\n    img = convert(Array{Float32},img) / 256 # scale into [0,1)\n    class = readbytes(label_f, n_data)\n    class = convert(Array{Float32},class)  We will permute the indices of the dataset so that they are stored in a shuffled ordering.  Then we iterate over the permuted indices and store the data and label values into the HDF5 dataset.      idx = 1:n_data      println(    $ idx ... )      idx = collect(idx)      rp = randperm(length(idx))      for j = 1:length(idx)        r_idx = rp[j]        dset_data[:,:,1,idx[j]] = img[(r_idx-1)*h*w+1:r_idx*h*w]        dset_label[1,idx[j]] = class[r_idx]      end", 
            "title": "Preparing the data"
        }, 
        {
            "location": "/man/examples/mnist/#the-model", 
            "text": "The model code can be found in  examples/mnist.jl  using Latte\n\nnet = Net(100)\ndata, label = HDF5DataLayer(net,  data/train.txt ,  data/test.txt )\nconv1    = ConvolutionLayer(:conv1, net, data, 20, 5, 1, 1)\nrelu1    = ReLULayer(:relu1, net, conv1)\npool1    = MaxPoolingLayer(:pool1, net, relu1, 2, 2, 0)\nconv2    = ConvolutionLayer(:conv2, net, pool1, 50, 5, 1, 1)\nrelu2    = ReLULayer(:relu2, net, conv2)\npool2    = MaxPoolingLayer(:pool2, net, relu2, 2, 2, 0)\nconv3    = ConvolutionLayer(:conv3, net, pool1, 50, 3, 1, 1)\nrelu3    = ReLULayer(:relu3, net, conv3)\npool3    = MaxPoolingLayer(:pool3, net, relu3, 2, 2, 0)\nfc4      = InnerProductLayer(:fc4, net, pool3, 512)\nrelu4    = ReLULayer(:relu4, net, fc4)\nfc5      = InnerProductLayer(:fc5, net, relu4, 512)\nrelu5    = ReLULayer(:relu5, net, fc5)\nfc6      = InnerProductLayer(:fc6, net, relu5, 10)\nloss     = SoftmaxLossLayer(:loss, net, fc6, label)\naccuracy = AccuracyLayer(:accuracy, net, fc6, label)\n\nparams = SolverParameters(\n    lr_policy    = LRPolicy.Inv(0.01, 0.0001, 0.75),\n    mom_policy   = MomPolicy.Fixed(0.9),\n    max_epoch    = 50,\n    regu_coef    = .0005)\nsgd = SGD(params)\nsolve(sgd, net)", 
            "title": "The Model"
        }, 
        {
            "location": "/man/examples/cifar/", 
            "text": "CIFAR-10\n\n\n\n\nTLDR\n\n\n$ \ncd\n ~/.julia/v0.4/Latte/examples/cifar10/data\n$ ./get-data.sh\n$ \ncd\n ..\n$ julia vgg-mini.jl", 
            "title": "CIFAR-10"
        }, 
        {
            "location": "/man/examples/cifar/#cifar-10", 
            "text": "", 
            "title": "CIFAR-10"
        }, 
        {
            "location": "/man/examples/cifar/#tldr", 
            "text": "$  cd  ~/.julia/v0.4/Latte/examples/cifar10/data\n$ ./get-data.sh\n$  cd  ..\n$ julia vgg-mini.jl", 
            "title": "TLDR"
        }, 
        {
            "location": "/lib/public/", 
            "text": "Public\n\n\n\n\nContents\n\n\n\n\nPublic\n\n\nContents\n\n\nIndex\n\n\nAPI\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nLatte.Connection\n\n\nLatte.Ensemble\n\n\nLatte.Net\n\n\nLatte.Net\n\n\nLatte.Param\n\n\nLatte.add_connections\n\n\nLatte.add_ensemble\n\n\nLatte.get_buffer\n\n\nLatte.init\n\n\nLatte.load_snapshot\n\n\nLatte.save_snapshot\n\n\nLatte.test\n\n\n\n\n\n\nAPI\n\n\n#\n\n\nLatte.Net\n \n \nType\n.\n\n\n\n\n Fields \n\n\n\n\nensembles\n          \u2013 a vector containing each \nEnsemble\n in the network.\n\n\nensembles_map\n      \u2013 a mapping between ensemble names in the network to the                           instance stored in \nensembles\n\n\nbuffers\n            \u2013 the internal buffers used by the network\n\n\ncurr_buffer_set\n    \u2013 the current buffer set (used for double buffering)\n\n\nforward_tasks\n      \u2013 a set of tasks for performing forward propogation of the network\n\n\nbackward_tasks\n     \u2013 a set of tasks for performing back propogation of the network\n\n\nupdate_tasks\n       \u2013 a set of tasks for performing parameter updates\n\n\nparams\n             \u2013 a vector of \nParam\n instances corresponding to network parameters\n\n\nrun_where\n          \u2013 DEPRECATED\n\n\nsignal\n             \u2013 DEPRECATED\n\n\nbatch_size\n         \u2013 the batch size of the network                            TODO: support different batch sizes for train/test\n\n\ntrain_epoch\n        \u2013 number of epochs completed for training\n\n\ntest_epoch\n         \u2013 number of epochs completed for testing\n\n\ncurr_time_step\n     \u2013 the current time step (for RNNs)\n\n\ntime_steps\n         \u2013 total number of time steps to unroll (for RNNs)\n\n\nnum_subgroups\n      \u2013 number of partitions in network (for model parallelism)\n\n\nensemble_send_list\n \u2013 a mapping between ensembles and a list of subgroups to send                           values to, used internally when synthesizing code for                           model parallelism\n\n\n\n\n#\n\n\nLatte.Net\n \n \nMethod\n.\n\n\n\n\nMain \nNet\n constructor that should be used. \n Params \n - \nbatch_size\n    \u2013 batch size of the network - \ntime_steps\n    \u2013 number of time steps to unroll the network (for RNNs) - \nnum_subgroups\n \u2013 number of subgroups in the network (for model parallelism)\n\n\n#\n\n\nLatte.Ensemble\n \n \nType\n.\n\n\n\n\nAn ensemble\n\n\n Fields \n\n\n\n\nname         \u2013 name of the ensemble\n\n\nneurons      \u2013 an array of neurons of type \nT\n\n\nconnections  \u2013 a list of \nEnsemble\ns connected to this ensemble\n\n\nbatch_fields \u2013 a vector of \nBatch\n fields for \nT\n (used internally)\n\n\narg_dim_info \u2013 \n\n\nparams       \u2013 a vector of \nParam\ns associated with the ensemble\n\n\nphase        \u2013 phase(s) in which this ensemble is active\n\n\nnet_subgroup \u2013 the net subgroup the ensemble is a member of (use for model parallelism)\n\n\n\n\n#\n\n\nLatte.Connection\n \n \nType\n.\n\n\n\n\nA connection between two ensembles.\n\n\n Fields \n\n\n\n\nsource        \u2013 the source \nEnsemble\n\n\nmapping       \u2013 a mapping function to a range of neurons in \nsource\n\n\nshape         \u2013 shape of the connected neurons returned by \nmapping\n\n\nsize          \u2013 length of the connected neurons returned by \nmapping\n\n\ncopy          \u2013 whether the connection requires input values to be copied\n\n\nis_dim_fixed  \u2013 vector of booleans that are true if the connection is fixed along a dimension\n\n\nis_one_to_one \u2013 whether the connection is one to one\n\n\npadding       \u2013 amount of padding used for the connection\n\n\nrecurrent     \u2013 whether the connection is recurrent\n\n\n\n\n#\n\n\nLatte.Param\n \n \nType\n.\n\n\n\n\nA parameter in a \nNet\n (learned during training)\n\n\n Fields \n\n\n\n\nname\n          \u2013 the name of the parameter\n\n\ngradient_name\n \u2013 the name of the gradient (should be \u2207\nname\n)\n\n\nhist_name\n     \u2013 the name of the history buffer (should be \nname\nhist)\n\n\nlearning_rate   \u2013 local learning rate for the parameter\n\n\nregu_coef       \u2013 local regularization coefficient\n\n\nclip_gradients  \u2013 NOT IMPLEMENTED, gradient clipping parameter\n\n\nvalue           \u2013 buffer containing the value of the parameter\n\n\ngradient        \u2013 buffer containing the gradient of the parameter\n\n\nhist            \u2013 buffer containing the history of the parameter\n\n\nrequest         \u2013 request id, used for MPI data parallelism\n\n\n\n\n#\n\n\nLatte.get_buffer\n \n \nMethod\n.\n\n\n\n\nGet a buffer at the time_step \nt\n\n\n Params \n\n\n\n\nnet\n   \u2013 network to get buffer\n\n\nname\n  \u2013 name of the buffer\n\n\nt\n     \u2013 time step\n\n\n\n\n#\n\n\nLatte.add_connections\n \n \nMethod\n.\n\n\n\n\nConnect neurons in \nsource\n to neurons in \nsink\n using the function \nmapping\n. \nmapping\n should be a function with a parameter for the index in each dimension of sink. For example, if sink is a 3-d ensemble, mapping = f(i, j, k) -\n ...\n\n\nmapping\n should return a tuple of continuous ranges corresponding to the indices of neurons in source that should be connected to the neuron at the current index\n\n\n#\n\n\nLatte.add_ensemble\n \n \nMethod\n.\n\n\n\n\nAdd an ensemble to the network \nnet\n\n\n#\n\n\nLatte.init\n \n \nMethod\n.\n\n\n\n\nInitialize the network \nnet\n.\n\n\nThis function begins by initializing each \nEnsemble\n in the Network.  This is done by calling \ninit(ens)\n which will be dispatched to the appropriate initialization routine.  These initialization routines are responsible for adding buffers to the network to contain neuron fields and output values.  See the specific initialization functions for different Ensemble types for more information (TODO: Reference these).  \n\n\nAfter initializaing the local fields and output values for each ensemble, we then initialize the input buffers for each ensemble.  This is done after all output values have been initialized so that we can analyze recurrent connections.  This is done by calling the \ninit_inputs(ens)\n routine.\n\n\n#\n\n\nLatte.load_snapshot\n \n \nMethod\n.\n\n\n\n\nLoad a network snapshot from \nfile\n.\n\n\nTODO: Can we save the structure of \nnet\n in the snapshot?\n\n\n#\n\n\nLatte.save_snapshot\n \n \nMethod\n.\n\n\n\n\nSave a snapshot of \nnet\n to \nfile\n\n\n#\n\n\nLatte.test\n \n \nMethod\n.\n\n\n\n\nTest \nnet\n for one epoch", 
            "title": "Public"
        }, 
        {
            "location": "/lib/public/#public", 
            "text": "", 
            "title": "Public"
        }, 
        {
            "location": "/lib/public/#contents", 
            "text": "Public  Contents  Index  API", 
            "title": "Contents"
        }, 
        {
            "location": "/lib/public/#index", 
            "text": "Latte.Connection  Latte.Ensemble  Latte.Net  Latte.Net  Latte.Param  Latte.add_connections  Latte.add_ensemble  Latte.get_buffer  Latte.init  Latte.load_snapshot  Latte.save_snapshot  Latte.test", 
            "title": "Index"
        }, 
        {
            "location": "/lib/public/#api", 
            "text": "#  Latte.Net     Type .    Fields    ensembles           \u2013 a vector containing each  Ensemble  in the network.  ensembles_map       \u2013 a mapping between ensemble names in the network to the                           instance stored in  ensembles  buffers             \u2013 the internal buffers used by the network  curr_buffer_set     \u2013 the current buffer set (used for double buffering)  forward_tasks       \u2013 a set of tasks for performing forward propogation of the network  backward_tasks      \u2013 a set of tasks for performing back propogation of the network  update_tasks        \u2013 a set of tasks for performing parameter updates  params              \u2013 a vector of  Param  instances corresponding to network parameters  run_where           \u2013 DEPRECATED  signal              \u2013 DEPRECATED  batch_size          \u2013 the batch size of the network                            TODO: support different batch sizes for train/test  train_epoch         \u2013 number of epochs completed for training  test_epoch          \u2013 number of epochs completed for testing  curr_time_step      \u2013 the current time step (for RNNs)  time_steps          \u2013 total number of time steps to unroll (for RNNs)  num_subgroups       \u2013 number of partitions in network (for model parallelism)  ensemble_send_list  \u2013 a mapping between ensembles and a list of subgroups to send                           values to, used internally when synthesizing code for                           model parallelism   #  Latte.Net     Method .   Main  Net  constructor that should be used.   Params   -  batch_size     \u2013 batch size of the network -  time_steps     \u2013 number of time steps to unroll the network (for RNNs) -  num_subgroups  \u2013 number of subgroups in the network (for model parallelism)  #  Latte.Ensemble     Type .   An ensemble   Fields    name         \u2013 name of the ensemble  neurons      \u2013 an array of neurons of type  T  connections  \u2013 a list of  Ensemble s connected to this ensemble  batch_fields \u2013 a vector of  Batch  fields for  T  (used internally)  arg_dim_info \u2013   params       \u2013 a vector of  Param s associated with the ensemble  phase        \u2013 phase(s) in which this ensemble is active  net_subgroup \u2013 the net subgroup the ensemble is a member of (use for model parallelism)   #  Latte.Connection     Type .   A connection between two ensembles.   Fields    source        \u2013 the source  Ensemble  mapping       \u2013 a mapping function to a range of neurons in  source  shape         \u2013 shape of the connected neurons returned by  mapping  size          \u2013 length of the connected neurons returned by  mapping  copy          \u2013 whether the connection requires input values to be copied  is_dim_fixed  \u2013 vector of booleans that are true if the connection is fixed along a dimension  is_one_to_one \u2013 whether the connection is one to one  padding       \u2013 amount of padding used for the connection  recurrent     \u2013 whether the connection is recurrent   #  Latte.Param     Type .   A parameter in a  Net  (learned during training)   Fields    name           \u2013 the name of the parameter  gradient_name  \u2013 the name of the gradient (should be \u2207 name )  hist_name      \u2013 the name of the history buffer (should be  name hist)  learning_rate   \u2013 local learning rate for the parameter  regu_coef       \u2013 local regularization coefficient  clip_gradients  \u2013 NOT IMPLEMENTED, gradient clipping parameter  value           \u2013 buffer containing the value of the parameter  gradient        \u2013 buffer containing the gradient of the parameter  hist            \u2013 buffer containing the history of the parameter  request         \u2013 request id, used for MPI data parallelism   #  Latte.get_buffer     Method .   Get a buffer at the time_step  t   Params    net    \u2013 network to get buffer  name   \u2013 name of the buffer  t      \u2013 time step   #  Latte.add_connections     Method .   Connect neurons in  source  to neurons in  sink  using the function  mapping .  mapping  should be a function with a parameter for the index in each dimension of sink. For example, if sink is a 3-d ensemble, mapping = f(i, j, k) -  ...  mapping  should return a tuple of continuous ranges corresponding to the indices of neurons in source that should be connected to the neuron at the current index  #  Latte.add_ensemble     Method .   Add an ensemble to the network  net  #  Latte.init     Method .   Initialize the network  net .  This function begins by initializing each  Ensemble  in the Network.  This is done by calling  init(ens)  which will be dispatched to the appropriate initialization routine.  These initialization routines are responsible for adding buffers to the network to contain neuron fields and output values.  See the specific initialization functions for different Ensemble types for more information (TODO: Reference these).    After initializaing the local fields and output values for each ensemble, we then initialize the input buffers for each ensemble.  This is done after all output values have been initialized so that we can analyze recurrent connections.  This is done by calling the  init_inputs(ens)  routine.  #  Latte.load_snapshot     Method .   Load a network snapshot from  file .  TODO: Can we save the structure of  net  in the snapshot?  #  Latte.save_snapshot     Method .   Save a snapshot of  net  to  file  #  Latte.test     Method .   Test  net  for one epoch", 
            "title": "API"
        }, 
        {
            "location": "/lib/internals/", 
            "text": "Internals\n\n\n\n\nContents\n\n\n\n\nInternals\n\n\nContents\n\n\nIndex\n\n\nNet\n\n\nConnections\n\n\nSynthesis and Optimization\n\n\nUtility Datastructures\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nLatte.Batch\n\n\nLatte.JuliaTask\n\n\nLatte.TaskSet\n\n\nLatte.UpdateTask\n\n\nLatte.add_forward_data_tasks\n\n\nLatte.add_forward_julia_tasks\n\n\nLatte.add_recv_expr\n\n\nLatte.add_send_exprs\n\n\nLatte.check_dimensions_fixed\n\n\nLatte.check_one_to_one\n\n\nLatte.gen_copy_block\n\n\nLatte.gen_neuron_backward\n\n\nLatte.gen_neuron_forward\n\n\nLatte.generate_c_function\n\n\nLatte.get_inner_loop_tiler\n\n\nLatte.get_src_idx\n\n\nLatte.get_tile_fusion_factor_backward\n\n\nLatte.get_tile_fusion_factor_forward\n\n\nLatte.get_tile_loops\n\n\nLatte.init_backward\n\n\nLatte.init_backward\n\n\nLatte.init_backward\n\n\nLatte.init_forward\n\n\nLatte.init_forward\n\n\nLatte.init_forward\n\n\nLatte.inner_loop_tiler\n\n\nLatte.is_tiled_loop\n\n\nLatte.optimize\n\n\nLatte.push_compute_tasks!\n\n\nLatte.tile_size_inliner\n\n\nLatte.unpack_tiled_loop\n\n\nLatte.update_tile_var\n\n\n\n\n\n\nNet\n\n\nThe \nNet\n datastructure is the main container used by the Latte compiler and runtime.  When constructing a \nNet\n a user adds ensembles and applies connections between them.  This implicitly constructs a task graph with connections as data dependencies and ensembles as groups of comptue tasks.  The \ninit(net::Net)\n routine is the main entry point for the Latte compiler. Inside this function, the Latte compiler consumes the implicit task graph, synthesizes functions to compute various tasks and optimizes these functions.\n\n\n{\ndocs\n}\n\n\ninit\n(\nnet\n:\n:Net\n)\n\n\n\ninit_buffer\n(\nnet\n:\n:Net\n,\n \nname\n:\n:Symbol\n,\n \nshape\n;\n \nfunc\n=\nzeros\n)\n\n\nset_buffer\n(\nnet\n:\n:Net\n,\n \nname\n:\n:Symbol\n,\n \narr\n:\n:Array\n;\n \n_copy\n=\ntrue\n)\n\n\nset_buffer\n(\nnet\n:\n:Net\n,\n \nname\n:\n:Symbol\n,\n \narr\n:\n:Array\n,\n \nt\n:\n:Int\n)\n\n\nget_buffer\n(\nnet\n:\n:Net\n,\n \nens\n:\n:AbstractEnsemble\n,\n \nname\n:\n:Symbol\n)\n\n\n\nrand_values\n(\nnet\n:\n:Net\n)\n\n\nclear_values\n(\nnet\n:\n:Net\n)\n\n\nclear_\n\u2207\n(\nnet\n:\n:Net\n)\n\n\n\n\n\n\n\n\nConnections\n\n\n#\n\n\nLatte.check_one_to_one\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.check_dimensions_fixed\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\n\n\nSynthesis and Optimization\n\n\n#\n\n\nLatte.add_send_exprs\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.add_recv_expr\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.init_forward\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\nTODO: doc\n\n\nTODO: doc\n\n\n#\n\n\nLatte.init_forward\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.init_forward\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.init_backward\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\nTODO: doc\n\n\nTODO: doc\n\n\n#\n\n\nLatte.init_backward\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.init_backward\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.add_forward_data_tasks\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.add_forward_julia_tasks\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.push_compute_tasks!\n \n \nMethod\n.\n\n\n\n\nnothing\n\n#\n\n\nLatte.generate_c_function\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.gen_neuron_backward\n \n \nMethod\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.gen_neuron_forward\n \n \nMethod\n.\n\n\n\n\nnothing\n\n#\n\n\nLatte.gen_copy_block\n \n \nMethod\n.\n\n\n\n\nSynthesize a loopnest to copy value or \u2207 to the appropriate buffer\n\n\n#\n\n\nLatte.get_src_idx\n \n \nMethod\n.\n\n\n\n\nExtract the final expression of the mapping function to be used as an indexing expression.  Handles cases where the final expression can be a Tuple or a single value.\n\n\n Params \n\n\n\n\nmapping\n \u2013 an ast for a mapping function\n\n\n\n\n#\n\n\nLatte.optimize\n \n \nMethod\n.\n\n\n\n\nOptimize a function \nfn\n\n\n Params \n\n\n\n\nargs\n                \u2013 an ordered vector of arguments (buffers)\n\n\ntile_fusion_factors\n \u2013 factors for tile fusions determined by connection                            structure\n\n\nfn\n                  \u2013 the ast of the function to be optimized\n\n\n\n\n#\n\n\nLatte.unpack_tiled_loop\n \n \nFunction\n.\n\n\n\n\nConvert from an internal Latte tiled_loop to a normal Julia :for expression\n\n\n#\n\n\nLatte.is_tiled_loop\n \n \nFunction\n.\n\n\n\n\nCheck if \nnode\n is an internal Latte tiled_loop\n\n\n#\n\n\nLatte.get_tile_fusion_factor_forward\n \n \nFunction\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.get_tile_fusion_factor_backward\n \n \nFunction\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.update_tile_var\n \n \nFunction\n.\n\n\n\n\nReplace indexing expressions with \ntile_var\n with the proper tiled expression When \ninputs\n are not copied, the neuron transformer appends :NOTILE to the index expression to force Latte not to tile the expression.\n\n\n#\n\n\nLatte.inner_loop_tiler\n \n \nFunction\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.get_inner_loop_tiler\n \n \nFunction\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.tile_size_inliner\n \n \nFunction\n.\n\n\n\n\nTODO: doc\n\n\n#\n\n\nLatte.get_tile_loops\n \n \nFunction\n.\n\n\n\n\nTile loop variables use \n_tile_idx\n loopvars.\n\n\n\n\nUtility Datastructures\n\n\n#\n\n\nLatte.TaskSet\n \n \nType\n.\n\n\n\n\nA container for tasks for multiple \nPhase\ns\n\n\n Fields \n\n\n\n\ntasks \u2013 a dictionary containing a \nVector\n of tasks for each \nPhase\n\n\n\n\n#\n\n\nLatte.JuliaTask\n \n \nType\n.\n\n\n\n\nA task that calls \nfunc\n with \nargs...\n\n\n#\n\n\nLatte.UpdateTask\n \n \nType\n.\n\n\n\n\nA task that updates parameter \nparam_id\n\n\n#\n\n\nLatte.Batch\n \n \nType\n.\n\n\n\n\nUsed in neuron definitions to mark a field to be unique per batch item", 
            "title": "Internals"
        }, 
        {
            "location": "/lib/internals/#internals", 
            "text": "", 
            "title": "Internals"
        }, 
        {
            "location": "/lib/internals/#contents", 
            "text": "Internals  Contents  Index  Net  Connections  Synthesis and Optimization  Utility Datastructures", 
            "title": "Contents"
        }, 
        {
            "location": "/lib/internals/#index", 
            "text": "Latte.Batch  Latte.JuliaTask  Latte.TaskSet  Latte.UpdateTask  Latte.add_forward_data_tasks  Latte.add_forward_julia_tasks  Latte.add_recv_expr  Latte.add_send_exprs  Latte.check_dimensions_fixed  Latte.check_one_to_one  Latte.gen_copy_block  Latte.gen_neuron_backward  Latte.gen_neuron_forward  Latte.generate_c_function  Latte.get_inner_loop_tiler  Latte.get_src_idx  Latte.get_tile_fusion_factor_backward  Latte.get_tile_fusion_factor_forward  Latte.get_tile_loops  Latte.init_backward  Latte.init_backward  Latte.init_backward  Latte.init_forward  Latte.init_forward  Latte.init_forward  Latte.inner_loop_tiler  Latte.is_tiled_loop  Latte.optimize  Latte.push_compute_tasks!  Latte.tile_size_inliner  Latte.unpack_tiled_loop  Latte.update_tile_var", 
            "title": "Index"
        }, 
        {
            "location": "/lib/internals/#net", 
            "text": "The  Net  datastructure is the main container used by the Latte compiler and runtime.  When constructing a  Net  a user adds ensembles and applies connections between them.  This implicitly constructs a task graph with connections as data dependencies and ensembles as groups of comptue tasks.  The  init(net::Net)  routine is the main entry point for the Latte compiler. Inside this function, the Latte compiler consumes the implicit task graph, synthesizes functions to compute various tasks and optimizes these functions.  { docs }  init ( net : :Net )  init_buffer ( net : :Net ,   name : :Symbol ,   shape ;   func = zeros )  set_buffer ( net : :Net ,   name : :Symbol ,   arr : :Array ;   _copy = true )  set_buffer ( net : :Net ,   name : :Symbol ,   arr : :Array ,   t : :Int )  get_buffer ( net : :Net ,   ens : :AbstractEnsemble ,   name : :Symbol )  rand_values ( net : :Net )  clear_values ( net : :Net )  clear_ \u2207 ( net : :Net )", 
            "title": "Net"
        }, 
        {
            "location": "/lib/internals/#connections", 
            "text": "#  Latte.check_one_to_one     Method .   TODO: doc  #  Latte.check_dimensions_fixed     Method .   TODO: doc", 
            "title": "Connections"
        }, 
        {
            "location": "/lib/internals/#synthesis-and-optimization", 
            "text": "#  Latte.add_send_exprs     Method .   TODO: doc  #  Latte.add_recv_expr     Method .   TODO: doc  #  Latte.init_forward     Method .   TODO: doc  TODO: doc  TODO: doc  #  Latte.init_forward     Method .   TODO: doc  #  Latte.init_forward     Method .   TODO: doc  #  Latte.init_backward     Method .   TODO: doc  TODO: doc  TODO: doc  #  Latte.init_backward     Method .   TODO: doc  #  Latte.init_backward     Method .   TODO: doc  #  Latte.add_forward_data_tasks     Method .   TODO: doc  #  Latte.add_forward_julia_tasks     Method .   TODO: doc  #  Latte.push_compute_tasks!     Method .   nothing #  Latte.generate_c_function     Method .   TODO: doc  #  Latte.gen_neuron_backward     Method .   TODO: doc  #  Latte.gen_neuron_forward     Method .   nothing #  Latte.gen_copy_block     Method .   Synthesize a loopnest to copy value or \u2207 to the appropriate buffer  #  Latte.get_src_idx     Method .   Extract the final expression of the mapping function to be used as an indexing expression.  Handles cases where the final expression can be a Tuple or a single value.   Params    mapping  \u2013 an ast for a mapping function   #  Latte.optimize     Method .   Optimize a function  fn   Params    args                 \u2013 an ordered vector of arguments (buffers)  tile_fusion_factors  \u2013 factors for tile fusions determined by connection                            structure  fn                   \u2013 the ast of the function to be optimized   #  Latte.unpack_tiled_loop     Function .   Convert from an internal Latte tiled_loop to a normal Julia :for expression  #  Latte.is_tiled_loop     Function .   Check if  node  is an internal Latte tiled_loop  #  Latte.get_tile_fusion_factor_forward     Function .   TODO: doc  #  Latte.get_tile_fusion_factor_backward     Function .   TODO: doc  #  Latte.update_tile_var     Function .   Replace indexing expressions with  tile_var  with the proper tiled expression When  inputs  are not copied, the neuron transformer appends :NOTILE to the index expression to force Latte not to tile the expression.  #  Latte.inner_loop_tiler     Function .   TODO: doc  #  Latte.get_inner_loop_tiler     Function .   TODO: doc  #  Latte.tile_size_inliner     Function .   TODO: doc  #  Latte.get_tile_loops     Function .   Tile loop variables use  _tile_idx  loopvars.", 
            "title": "Synthesis and Optimization"
        }, 
        {
            "location": "/lib/internals/#utility-datastructures", 
            "text": "#  Latte.TaskSet     Type .   A container for tasks for multiple  Phase s   Fields    tasks \u2013 a dictionary containing a  Vector  of tasks for each  Phase   #  Latte.JuliaTask     Type .   A task that calls  func  with  args...  #  Latte.UpdateTask     Type .   A task that updates parameter  param_id  #  Latte.Batch     Type .   Used in neuron definitions to mark a field to be unique per batch item", 
            "title": "Utility Datastructures"
        }
    ]
}